# services/data_service.py

import asyncio
import json
import logging
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import os

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å pandas
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

logger = logging.getLogger(__name__)

class DataServiceConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    DATA_DIR = Path(os.getenv('DATA_DIR', 'data'))
    BACKUP_DIR = Path(os.getenv('BACKUP_DIR', 'backups'))
    EXPORT_DIR = Path(os.getenv('EXPORT_DIR', 'exports'))
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∞
    MAX_USERS_CACHE = int(os.getenv('MAX_USERS_CACHE', 1000))
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±—ç–∫–∞–ø–æ–≤
    BACKUP_INTERVAL_HOURS = int(os.getenv('BACKUP_INTERVAL_HOURS', 6))
    MAX_BACKUPS_KEEP = int(os.getenv('MAX_BACKUPS_KEEP', 10))
    
    # –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö
    USERS_DATA_FILE = 'users_data.json'
    ANALYTICS_DATA_FILE = 'analytics_data.json'
    SYSTEM_LOG_FILE = 'system_log.json'
    
    @classmethod
    def ensure_directories(cls):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        for directory in [cls.DATA_DIR, cls.BACKUP_DIR, cls.EXPORT_DIR]:
            directory.mkdir(exist_ok=True)
            logger.debug(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ–∑–¥–∞–Ω–∞/–ø—Ä–æ–≤–µ—Ä–µ–Ω–∞: {directory}")

class DataService:
    """
    –ü–æ–ª–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–æ–≤
    - –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
    - –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏
    - –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    """
    
    def __init__(self, data_file: str = None):
        self.config = DataServiceConfig()
        self.config.ensure_directories()
        
        # –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö
        self.data_file = self.config.DATA_DIR / (data_file or self.config.USERS_DATA_FILE)
        self.analytics_file = self.config.DATA_DIR / self.config.ANALYTICS_DATA_FILE
        self.system_log_file = self.config.DATA_DIR / self.config.SYSTEM_LOG_FILE
        
        # –ö—ç—à –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        self.users_cache: Dict[int, Any] = {}  # Any = User class from main.py
        self.cache_lock = threading.RLock()
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.last_save_time = time.time()
        self.pending_saves = set()  # user_ids –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.total_operations = 0
        self.failed_operations = 0
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self._initialize()
        
    def _initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataService...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            self._load_all_users()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            self._load_system_data()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
            self._start_background_tasks()
            
            logger.info(f"‚úÖ DataService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∫—ç—à–µ: {len(self.users_cache)}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DataService: {e}")
            raise
    
    def _load_all_users(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if not self.data_file.exists():
                logger.info("üìÇ –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–π –±–∞–∑—ã")
                self.users_cache = {}
                return
            
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, dict):
                logger.warning("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö")
                self.users_cache = {}
                return
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∫—ç—à (—Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤–∏–¥)
            loaded_count = 0
            with self.cache_lock:
                for user_id_str, user_data in data.items():
                    try:
                        user_id = int(user_id_str)
                        # –ü–æ–∫–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å, –ø–æ–∑–∂–µ User –∫–ª–∞—Å—Å –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç
                        self.users_cache[user_id] = user_data
                        loaded_count += 1
                    except (ValueError, TypeError) as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id_str}: {e}")
            
            logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ {self.data_file}")
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            self._create_backup_and_reset()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.users_cache = {}
    
    def _create_backup_and_reset(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ —Å–±—Ä–æ—Å"""
        try:
            if self.data_file.exists():
                backup_name = f"corrupted_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                backup_path = self.config.BACKUP_DIR / backup_name
                self.data_file.replace(backup_path)
                logger.warning(f"üîÑ –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–µ—Ä–µ–º–µ—â–µ–Ω –≤ {backup_path}")
            
            self.users_cache = {}
            logger.info("üÜï –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —á–∏—Å—Ç–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–∞: {e}")
    
    def _load_system_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É
            if self.analytics_file.exists():
                with open(self.analytics_file, 'r', encoding='utf-8') as f:
                    analytics_data = json.load(f)
                logger.debug(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞: {len(analytics_data)} –∑–∞–ø–∏—Å–µ–π")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ª–æ–≥–∏
            if self.system_log_file.exists():
                with open(self.system_log_file, 'r', encoding='utf-8') as f:
                    system_data = json.load(f)
                logger.debug(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Å–∏—Å—Ç–µ–º–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(system_data)} –∑–∞–ø–∏—Å–µ–π")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _start_background_tasks(self):
        """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á"""
        try:
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å APScheduler –∏–ª–∏ asyncio tasks –¥–ª—è:
            # - –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            # - –°–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–æ–≤
            # - –û—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤
            logger.debug("üîÑ –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∑–∞–ø—É—â–µ–Ω—ã")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á: {e}")
    
    # ===== –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ –†–ê–ë–û–¢–´ –° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø–ú–ò =====
    
    def get_user_data(self, user_id: int) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ ID"""
        with self.cache_lock:
            user_data = self.users_cache.get(user_id)
            if user_data:
                self.total_operations += 1
                logger.debug(f"üë§ –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return user_data
    
    def save_user_data(self, user_id: int, user_data: Dict):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∫—ç—à"""
        try:
            with self.cache_lock:
                self.users_cache[user_id] = user_data
                self.pending_saves.add(user_id)
                self.total_operations += 1
                
            logger.debug(f"üíæ –î–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –æ–±–Ω–æ–≤–ª–µ–Ω—ã –≤ –∫—ç—à–µ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
            self.failed_operations += 1
            raise
    
    def delete_user_data(self, user_id: int) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            with self.cache_lock:
                if user_id in self.users_cache:
                    del self.users_cache[user_id]
                    self.pending_saves.add(user_id)  # –î–ª—è —Ñ–∏–∫—Å–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è
                    logger.info(f"üóëÔ∏è –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} —É–¥–∞–ª–µ–Ω")
                    return True
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
            self.failed_operations += 1
            return False
    
    def user_exists(self, user_id: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        with self.cache_lock:
            return user_id in self.users_cache
    
    def get_all_users_data(self) -> Dict[int, Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        with self.cache_lock:
            return self.users_cache.copy()
    
    def get_users_count(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        return len(self.users_cache)
    
    # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –ù–ê –î–ò–°–ö =====
    
    def save_all_to_disk(self) -> bool:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫"""
        try:
            start_time = time.time()
            
            with self.cache_lock:
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                if self.data_file.exists():
                    backup_name = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    backup_path = self.config.BACKUP_DIR / backup_name
                    self.data_file.replace(backup_path)
                    logger.debug(f"üíæ –°–æ–∑–¥–∞–Ω –±—ç–∫–∞–ø: {backup_name}")
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫—ç—à –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                data_to_save = {}
                for user_id, user_data in self.users_cache.items():
                    data_to_save[str(user_id)] = user_data
                
                # –ê—Ç–æ–º–∞—Ä–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                temp_file = self.data_file.with_suffix('.tmp')
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(data_to_save, f, ensure_ascii=False, indent=2)
                
                # –ó–∞–º–µ–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
                temp_file.replace(self.data_file)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                self.last_save_time = time.time()
                self.pending_saves.clear()
                
                save_duration = time.time() - start_time
                logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ –∑–∞ {save_duration:.2f}—Å ({len(self.users_cache)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)")
                
                return True
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.failed_operations += 1
            return False
    
    async def save_all_to_disk_async(self) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.save_all_to_disk)
    
    def force_save(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        logger.info("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        return self.save_all_to_disk()
    
    # ===== –≠–ö–°–ü–û–†–¢ –î–ê–ù–ù–´–• =====
    
    def export_user_data(self, user_id: int, format: str = "json") -> Optional[bytes]:
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        try:
            user_data = self.get_user_data(user_id)
            if not user_data:
                logger.warning(f"‚ö†Ô∏è –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
                return None
            
            if format.lower() == "json":
                export_data = {
                    "export_info": {
                        "format": "json",
                        "version": "4.0",
                        "exported_at": datetime.now().isoformat(),
                        "user_id": user_id
                    },
                    "user_data": user_data
                }
                
                json_str = json.dumps(export_data, ensure_ascii=False, indent=2)
                logger.info(f"üì§ JSON —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω")
                return json_str.encode('utf-8')
            
            elif format.lower() == "csv" and PANDAS_AVAILABLE:
                # –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–¥–∞—á –≤ CSV
                tasks_data = []
                
                if "tasks" in user_data:
                    for task_id, task_info in user_data["tasks"].items():
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏
                        completions = task_info.get("completions", [])
                        for completion in completions:
                            tasks_data.append({
                                "task_id": task_id,
                                "title": task_info.get("title", ""),
                                "category": task_info.get("category", ""),
                                "priority": task_info.get("priority", ""),
                                "status": task_info.get("status", ""),
                                "date": completion.get("date", ""),
                                "completed": completion.get("completed", False),
                                "time_spent": completion.get("time_spent"),
                                "note": completion.get("note", ""),
                                "timestamp": completion.get("timestamp", "")
                            })
                
                if tasks_data:
                    df = pd.DataFrame(tasks_data)
                    csv_data = df.to_csv(index=False)
                    logger.info(f"üìä CSV —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω ({len(tasks_data)} –∑–∞–ø–∏—Å–µ–π)")
                    return csv_data.encode('utf-8')
                else:
                    # –ü—É—Å—Ç–æ–π CSV —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                    headers = "task_id,title,category,priority,status,date,completed,time_spent,note,timestamp\n"
                    return headers.encode('utf-8')
            
            elif format.lower() == "xlsx" and PANDAS_AVAILABLE:
                # –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel
                import io
                
                tasks_data = []
                stats_data = []
                
                # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á
                if "tasks" in user_data:
                    for task_id, task_info in user_data["tasks"].items():
                        completions = task_info.get("completions", [])
                        for completion in completions:
                            tasks_data.append({
                                "task_id": task_id,
                                "title": task_info.get("title", ""),
                                "category": task_info.get("category", ""),
                                "priority": task_info.get("priority", ""),
                                "status": task_info.get("status", ""),
                                "date": completion.get("date", ""),
                                "completed": completion.get("completed", False),
                                "time_spent": completion.get("time_spent"),
                                "note": completion.get("note", "")
                            })
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                if "stats" in user_data:
                    stats = user_data["stats"]
                    stats_data.append({
                        "metric": "–í—Å–µ–≥–æ –∑–∞–¥–∞—á",
                        "value": stats.get("total_tasks", 0)
                    })
                    stats_data.append({
                        "metric": "–í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–¥–∞—á",
                        "value": stats.get("completed_tasks", 0)
                    })
                    stats_data.append({
                        "metric": "–¢–µ–∫—É—â–∏–π streak",
                        "value": stats.get("current_streak", 0)
                    })
                    stats_data.append({
                        "metric": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π streak",
                        "value": stats.get("longest_streak", 0)
                    })
                    stats_data.append({
                        "metric": "–û–±—â–∏–π XP",
                        "value": stats.get("total_xp", 0)
                    })
                    stats_data.append({
                        "metric": "–£—Ä–æ–≤–µ–Ω—å",
                        "value": stats.get("level", 1)
                    })
                
                # –°–æ–∑–¥–∞–µ–º Excel —Ñ–∞–π–ª
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    if tasks_data:
                        df_tasks = pd.DataFrame(tasks_data)
                        df_tasks.to_excel(writer, sheet_name='–ó–∞–¥–∞—á–∏', index=False)
                    
                    if stats_data:
                        df_stats = pd.DataFrame(stats_data)
                        df_stats.to_excel(writer, sheet_name='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', index=False)
                
                logger.info(f"üìà Excel —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω")
                return output.getvalue()
            
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞: {format}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
            return None
    
    def export_all_users_data(self, format: str = "json") -> Optional[bytes]:
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        try:
            if format.lower() == "json":
                export_data = {
                    "export_info": {
                        "format": "json",
                        "version": "4.0", 
                        "exported_at": datetime.now().isoformat(),
                        "total_users": len(self.users_cache)
                    },
                    "users_data": self.users_cache
                }
                
                json_str = json.dumps(export_data, ensure_ascii=False, indent=2)
                logger.info(f"üì§ –ü–æ–ª–Ω—ã–π JSON —ç–∫—Å–ø–æ—Ä—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω ({len(self.users_cache)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)")
                return json_str.encode('utf-8')
            
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∞: {format}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
            return None
    
    # ===== –ë–≠–ö–ê–ü–´ –ò –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï =====
    
    def create_backup(self, backup_name: str = None) -> Optional[Path]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if not backup_name:
                backup_name = f"manual_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            backup_path = self.config.BACKUP_DIR / backup_name
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö
            if self.data_file.exists():
                import shutil
                shutil.copy2(self.data_file, backup_path)
                logger.info(f"üíæ –ë—ç–∫–∞–ø —Å–æ–∑–¥–∞–Ω: {backup_path}")
                return backup_path
            else:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–∞")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–∞: {e}")
            return None
    
    def restore_from_backup(self, backup_path: Path) -> bool:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ –±—ç–∫–∞–ø–∞"""
        try:
            if not backup_path.exists():
                logger.error(f"‚ùå –§–∞–π–ª –±—ç–∫–∞–ø–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_path}")
                return False
            
            # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            current_backup = self.create_backup("pre_restore_backup.json")
            if current_backup:
                logger.info(f"üíæ –¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {current_backup}")
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –±—ç–∫–∞–ø–∞
            import shutil
            shutil.copy2(backup_path, self.data_file)
            
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            self._load_all_users()
            
            logger.info(f"‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ {backup_path} –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑ –±—ç–∫–∞–ø–∞: {e}")
            return False
    
    def cleanup_old_backups(self, keep_count: int = None):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤"""
        try:
            if keep_count is None:
                keep_count = self.config.MAX_BACKUPS_KEEP
            
            backups = list(self.config.BACKUP_DIR.glob("backup_*.json"))
            backups.extend(self.config.BACKUP_DIR.glob("manual_backup_*.json"))
            
            if len(backups) <= keep_count:
                logger.debug(f"üìÅ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—ç–∫–∞–ø–æ–≤ ({len(backups)}) –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({keep_count})")
                return
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
            backups.sort(key=lambda x: x.stat().st_mtime)
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ
            to_delete = backups[:-keep_count]
            deleted_count = 0
            
            for backup in to_delete:
                try:
                    backup.unlink()
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –±—ç–∫–∞–ø–∞ {backup}: {e}")
            
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –±—ç–∫–∞–ø–æ–≤: {e}")
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤"""
        try:
            backups = []
            backup_files = list(self.config.BACKUP_DIR.glob("*.json"))
            
            for backup_file in backup_files:
                stat = backup_file.stat()
                backups.append({
                    "name": backup_file.name,
                    "path": backup_file,
                    "size": stat.st_size,
                    "created": datetime.fromtimestamp(stat.st_ctime),
                    "modified": datetime.fromtimestamp(stat.st_mtime)
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
            backups.sort(key=lambda x: x["created"], reverse=True)
            
            return backups
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –±—ç–∫–∞–ø–æ–≤: {e}")
            return []
    
    # ===== –ê–ù–ê–õ–ò–¢–ò–ö–ê –ò –ú–ï–¢–†–ò–ö–ò =====
    
    def get_service_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "users_count": len(self.users_cache),
            "pending_saves": len(self.pending_saves),
            "total_operations": self.total_operations,
            "failed_operations": self.failed_operations,
            "last_save_time": self.last_save_time,
            "cache_size_mb": len(str(self.users_cache)) / 1024 / 1024,
            "data_file_size": self.data_file.stat().st_size if self.data_file.exists() else 0,
            "backups_count": len(list(self.config.BACKUP_DIR.glob("*.json")))
        }
    
    def get_users_analytics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º"""
        try:
            analytics = {
                "total_users": len(self.users_cache),
                "active_users": 0,
                "total_tasks": 0,
                "completed_tasks": 0,
                "total_xp": 0,
                "avg_level": 0,
                "top_users_by_level": [],
                "users_by_registration_date": {},
                "tasks_by_category": {},
                "completion_rate": 0
            }
            
            levels = []
            registration_dates = []
            
            for user_data in self.users_cache.values():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–µ—Å—Ç—å –∑–∞–¥–∞—á–∏)
                if user_data.get("tasks"):
                    analytics["active_users"] += 1
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á–∏
                tasks = user_data.get("tasks", {})
                analytics["total_tasks"] += len(tasks)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏
                for task_data in tasks.values():
                    category = task_data.get("category", "unknown")
                    analytics["tasks_by_category"][category] = analytics["tasks_by_category"].get(category, 0) + 1
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
                    completions = task_data.get("completions", [])
                    completed = sum(1 for c in completions if c.get("completed", False))
                    analytics["completed_tasks"] += completed
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                stats = user_data.get("stats", {})
                analytics["total_xp"] += stats.get("total_xp", 0)
                
                level = stats.get("level", 1)
                levels.append(level)
                
                # –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
                reg_date = stats.get("registration_date", "")
                if reg_date:
                    reg_day = reg_date[:10]  # YYYY-MM-DD
                    analytics["users_by_registration_date"][reg_day] = analytics["users_by_registration_date"].get(reg_day, 0) + 1
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
            if levels:
                analytics["avg_level"] = sum(levels) / len(levels)
            
            if analytics["total_tasks"] > 0:
                analytics["completion_rate"] = (analytics["completed_tasks"] / analytics["total_tasks"]) * 100
            
            logger.debug(f"üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è {analytics['total_users']} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
            return analytics
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
            return {}
    
    def save_analytics_snapshot(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–Ω–∏–º–∫–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        try:
            analytics = self.get_users_analytics()
            service_metrics = self.get_service_metrics()
            
            snapshot = {
                "timestamp": datetime.now().isoformat(),
                "analytics": analytics,
                "service_metrics": service_metrics
            }
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–Ω–∏–º–∫–∏
            snapshots = []
            if self.analytics_file.exists():
                with open(self.analytics_file, 'r', encoding='utf-8') as f:
                    snapshots = json.load(f)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Å–Ω–∏–º–æ–∫
            snapshots.append(snapshot)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–Ω–∏–º–∫–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100)
            if len(snapshots) > 100:
                snapshots = snapshots[-100:]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            with open(self.analytics_file, 'w', encoding='utf-8') as f:
                json.dump(snapshots, f, ensure_ascii=False, indent=2)
            
            logger.info("üìä –°–Ω–∏–º–æ–∫ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
    
    # ===== –ü–û–ò–°–ö –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø =====
    
    def search_users(self, query: str, field: str = "all") -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        try:
            results = []
            query_lower = query.lower()
            
            for user_id, user_data in self.users_cache.items():
                match = False
                
                if field == "all" or field == "username":
                    username = user_data.get("username", "")
                    if username and query_lower in username.lower():
                        match = True
                
                if field == "all" or field == "first_name":
                    first_name = user_data.get("first_name", "")
                    if first_name and query_lower in first_name.lower():
                        match = True
                
                if field == "all" or field == "task_title":
                    tasks = user_data.get("tasks", {})
                    for task in tasks.values():
                        if query_lower in task.get("title", "").lower():
                            match = True
                            break
                
                if match:
                    results.append({
                        "user_id": user_id,
                        "username": user_data.get("username"),
                        "first_name": user_data.get("first_name"),
                        "tasks_count": len(user_data.get("tasks", {})),
                        "level": user_data.get("stats", {}).get("level", 1)
                    })
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}'")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def filter_users_by_criteria(self, criteria: Dict[str, Any]) -> List[int]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        try:
            filtered_users = []
            
            for user_id, user_data in self.users_cache.items():
                match = True
                
                # –§–∏–ª—å—Ç—Ä –ø–æ —É—Ä–æ–≤–Ω—é
                if "min_level" in criteria:
                    user_level = user_data.get("stats", {}).get("level", 1)
                    if user_level < criteria["min_level"]:
                        match = False
                
                if "max_level" in criteria:
                    user_level = user_data.get("stats", {}).get("level", 1)
                    if user_level > criteria["max_level"]:
                        match = False
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–¥–∞—á
                if "min_tasks" in criteria:
                    tasks_count = len(user_data.get("tasks", {}))
                    if tasks_count < criteria["min_tasks"]:
                        match = False
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                if "has_tasks" in criteria and criteria["has_tasks"]:
                    if not user_data.get("tasks"):
                        match = False
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
                if "registered_after" in criteria:
                    reg_date = user_data.get("stats", {}).get("registration_date")
                    if not reg_date or reg_date < criteria["registered_after"]:
                        match = False
                
                if match:
                    filtered_users.append(user_id)
            
            logger.info(f"üîç –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(filtered_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
            return filtered_users
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {e}")
            return []
    
    # ===== –í–ê–õ–ò–î–ê–¶–ò–Ø –ò –û–ë–°–õ–£–ñ–ò–í–ê–ù–ò–ï =====
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            report = {
                "total_users": len(self.users_cache),
                "valid_users": 0,
                "invalid_users": [],
                "orphaned_data": [],
                "missing_fields": [],
                "data_inconsistencies": []
            }
            
            for user_id, user_data in self.users_cache.items():
                is_valid = True
                user_issues = []
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                required_fields = ["user_id", "username", "first_name"]
                for field in required_fields:
                    if field not in user_data:
                        user_issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ {field}")
                        is_valid = False
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ user_id
                if user_data.get("user_id") != user_id:
                    user_issues.append("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ user_id")
                    is_valid = False
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–¥–∞—á
                tasks = user_data.get("tasks", {})
                if tasks and not isinstance(tasks, dict):
                    user_issues.append("–ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ tasks")
                    is_valid = False
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats = user_data.get("stats", {})
                if stats:
                    if stats.get("total_tasks", 0) != len(tasks):
                        user_issues.append("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ total_tasks –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–¥–∞—á")
                        report["data_inconsistencies"].append({
                            "user_id": user_id,
                            "issue": "total_tasks mismatch",
                            "expected": len(tasks),
                            "actual": stats.get("total_tasks", 0)
                        })
                
                if is_valid:
                    report["valid_users"] += 1
                else:
                    report["invalid_users"].append({
                        "user_id": user_id,
                        "issues": user_issues
                    })
            
            logger.info(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {report['valid_users']}/{report['total_users']} –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
            return report
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏: {e}")
            return {"error": str(e)}
    
    def repair_data_inconsistencies(self) -> Dict[str, int]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –¥–∞–Ω–Ω—ã—Ö"""
        try:
            repairs = {
                "total_tasks_fixed": 0,
                "missing_stats_added": 0,
                "invalid_data_removed": 0
            }
            
            with self.cache_lock:
                for user_id, user_data in self.users_cache.items():
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º total_tasks
                    tasks = user_data.get("tasks", {})
                    stats = user_data.setdefault("stats", {})
                    
                    if stats.get("total_tasks", 0) != len(tasks):
                        stats["total_tasks"] = len(tasks)
                        repairs["total_tasks_fixed"] += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    default_stats = {
                        "completed_tasks": 0,
                        "current_streak": 0,
                        "longest_streak": 0,
                        "total_xp": 0,
                        "level": 1,
                        "registration_date": datetime.now().isoformat()
                    }
                    
                    for field, default_value in default_stats.items():
                        if field not in stats:
                            stats[field] = default_value
                            repairs["missing_stats_added"] += 1
                    
                    self.pending_saves.add(user_id)
            
            logger.info(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {repairs}")
            return repairs
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return {}
    
    def optimize_storage(self) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            result = {
                "before_size": 0,
                "after_size": 0,
                "removed_empty_fields": 0,
                "compressed_data": 0
            }
            
            # –†–∞–∑–º–µ—Ä –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            result["before_size"] = len(str(self.users_cache))
            
            with self.cache_lock:
                for user_id, user_data in self.users_cache.items():
                    modified = False
                    
                    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –ø–æ–ª—è
                    def remove_empty_fields(obj):
                        if isinstance(obj, dict):
                            return {k: remove_empty_fields(v) for k, v in obj.items() 
                                   if v is not None and v != "" and v != []}
                        elif isinstance(obj, list):
                            return [remove_empty_fields(item) for item in obj if item]
                        return obj
                    
                    cleaned_data = remove_empty_fields(user_data)
                    if len(str(cleaned_data)) < len(str(user_data)):
                        self.users_cache[user_id] = cleaned_data
                        result["removed_empty_fields"] += 1
                        modified = True
                    
                    if modified:
                        self.pending_saves.add(user_id)
            
            # –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            result["after_size"] = len(str(self.users_cache))
            result["size_reduction"] = result["before_size"] - result["after_size"]
            
            logger.info(f"‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            return {}
    
    # ===== –£–¢–ò–õ–ò–¢–´ –ò –°–ï–†–í–ò–°–ù–´–ï –ú–ï–¢–û–î–´ =====
    
    def get_storage_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        try:
            info = {
                "data_file": {
                    "path": str(self.data_file),
                    "exists": self.data_file.exists(),
                    "size": self.data_file.stat().st_size if self.data_file.exists() else 0,
                    "modified": datetime.fromtimestamp(self.data_file.stat().st_mtime).isoformat() if self.data_file.exists() else None
                },
                "cache": {
                    "users_count": len(self.users_cache),
                    "pending_saves": len(self.pending_saves),
                    "memory_usage": len(str(self.users_cache))
                },
                "backups": {
                    "directory": str(self.config.BACKUP_DIR),
                    "count": len(list(self.config.BACKUP_DIR.glob("*.json"))),
                    "total_size": sum(f.stat().st_size for f in self.config.BACKUP_DIR.glob("*.json"))
                },
                "metrics": self.get_service_metrics()
            }
            
            return info
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {e}")
            return {}
    
    def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            health = {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "checks": {}
            }
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
            try:
                health["checks"]["data_file"] = {
                    "status": "ok" if self.data_file.exists() else "warning",
                    "message": "–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç—É–ø–µ–Ω" if self.data_file.exists() else "–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω"
                }
            except Exception as e:
                health["checks"]["data_file"] = {"status": "error", "message": str(e)}
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            try:
                cache_size = len(self.users_cache)
                health["checks"]["cache"] = {
                    "status": "ok" if cache_size < self.config.MAX_USERS_CACHE else "warning",
                    "message": f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∫—ç—à–µ: {cache_size}"
                }
            except Exception as e:
                health["checks"]["cache"] = {"status": "error", "message": str(e)}
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø–µ—Ä–∞—Ü–∏–π
            error_rate = (self.failed_operations / max(self.total_operations, 1)) * 100
            health["checks"]["operations"] = {
                "status": "ok" if error_rate < 5 else "warning" if error_rate < 10 else "error",
                "message": f"–û—à–∏–±–æ–∫: {error_rate:.1f}% ({self.failed_operations}/{self.total_operations})"
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
            statuses = [check["status"] for check in health["checks"].values()]
            if "error" in statuses:
                health["status"] = "error"
            elif "warning" in statuses:
                health["status"] = "warning"
            
            return health
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
            return {
                "status": "error",
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            logger.info("üõë –ó–∞–∫—Ä—ã—Ç–∏–µ DataService...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            if self.pending_saves:
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(self.pending_saves)} –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π...")
                self.save_all_to_disk()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–Ω–∏–º–æ–∫ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
            self.save_analytics_snapshot()
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à
            with self.cache_lock:
                self.users_cache.clear()
                self.pending_saves.clear()
            
            logger.info("‚úÖ DataService –∑–∞–∫—Ä—ã—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è DataService: {e}")
    
    def __enter__(self):
        """Context manager –≤—Ö–æ–¥"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager –≤—ã—Ö–æ–¥"""
        self.close()

# ===== –ì–õ–û–ë–ê–õ–¨–ù–´–ô –≠–ö–ó–ï–ú–ü–õ–Ø–† =====

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
_global_data_service = None

def get_data_service() -> DataService:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä DataService"""
    global _global_data_service
    if _global_data_service is None:
        _global_data_service = DataService()
    return _global_data_service

def initialize_data_service(data_file: str = None) -> DataService:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ DataService"""
    global _global_data_service
    _global_data_service = DataService(data_file)
    return _global_data_service

def close_data_service():
    """–ó–∞–∫—Ä—ã—Ç–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ DataService"""
    global _global_data_service
    if _global_data_service:
        _global_data_service.close()
        _global_data_service = None
